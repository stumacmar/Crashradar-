name: Build FRED cache (always produce valid JSON)

on:
  workflow_dispatch: {}
  schedule:
    - cron: "7 6 * * 1-5"   # 06:07 UTC on weekdays
  push:
    branches: [ main ]
    paths:
      - ".github/workflows/fred-cache.yml"
      - "data/**"
      - "index.html"

permissions:
  contents: write

jobs:
  update-cache:
    runs-on: ubuntu-latest

    steps:
      - name: Check out repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Build data/fred_cache.json
        env:
          FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, json, datetime as dt
          from urllib.request import urlopen
          from urllib.parse import urlencode

          OUT_FILE = "data/fred_cache.json"
          os.makedirs("data", exist_ok=True)

          series_map = {
              "T10Y3M": {"id": "T10Y3M", "fmt": float},        # Yield curve
              "CREDIT": {"id": "BAA10YM", "fmt": float},       # Credit spread proxy
              "UN_SLOPE6": {"id": "UNRATE", "fmt": float},     # Unemployment rate (derive slope)
              "DD_12M": {"id": "SP500", "fmt": float},         # SP500 (derive drawdown)
              "NFCI": {"id": "NFCI", "fmt": float},            # Financial conditions
              "VIX_PROXY": {"id": "VIXCLS", "fmt": float},     # Volatility index
              "RSAFS": {"id": "RSAFS", "fmt": float},          # Retail sales (derive momentum)
              "UMCSENT": {"id": "UMCSENT", "fmt": float}       # Consumer sentiment
          }

          def fred_json(series_id, api_key):
              params = dict(series_id=series_id, api_key=api_key,
                            file_type="json", observation_start="2010-01-01")
              url = "https://api.stlouisfed.org/fred/series/observations?" + urlencode(params)
              with urlopen(url, timeout=30) as r:
                  return json.load(r)

          def to_obs_list(raw):
              out = []
              for o in raw.get("observations", []):
                  v = o.get("value")
                  if not v or v in (".", "NaN"):
                      continue
                  try:
                      fv = float(v)
                  except:
                      continue
                  out.append({"date": o["date"][:10], "value": f"{fv}"})
              return out

          api_key = os.environ.get("FRED_API_KEY", "").strip()
          use_fred = bool(api_key)
          series = {}

          try:
              if not use_fred:
                  raise RuntimeError("No FRED_API_KEY set")

              raw = {k: fred_json(v["id"], api_key) for k, v in series_map.items()}
              conv = {k: to_obs_list(raw[k]) for k in raw}

              # Derive unemployment 6m slope
              def slope6(obs):
                  by_month = {}
                  for o in obs:
                      by_month[o["date"][:7]] = float(o["value"])
                  months = sorted(by_month)
                  out = []
                  for i in range(6, len(months)):
                      m, prev = months[i], months[i-6]
                      delta = by_month[m] - by_month[prev]
                      y, mo = map(int, m.split("-"))
                      d = dt.date(y, mo, 1)
                      out.append({"date": str(d), "value": f"{delta}"})
                  return out

              # Derive 12-month drawdown (S&P 500)
              def drawdown12(obs):
                  vals = [float(o["value"]) for o in obs]
                  peak = []
                  for i in range(len(vals)):
                      p = max(vals[max(0, i-252):i+1])
                      peak.append(p)
                  out = []
                  for o, p in zip(obs, peak):
                      if p == 0: continue
                      out.append({"date": o["date"], "value": f"{(vals[obs.index(o)]/p)-1.0}"})
                  return out

              series = {
                  "T10Y3M": {"name": "10-Year minus 3-Month Treasury Spread", "observations": conv["T10Y3M"][-365:]},
                  "CREDIT": {"name": "Credit Spread (BAA - 10y)", "observations": conv["CREDIT"][-365:]},
                  "UN_SLOPE6": {"name": "Unemployment 6m Slope", "observations": slope6(conv["UN_SLOPE6"])[-365:]},
                  "DD_12M": {"name": "S&P 500 12m Drawdown", "observations": drawdown12(conv["DD_12M"])[-365:]},
                  "NFCI": {"name": "Chicago Fed NFCI", "observations": conv["NFCI"][-365:]},
                  "VIX_PROXY": {"name": "Volatility Index (VIX)", "observations": conv["VIX_PROXY"][-365:]},
                  "RSAFS": {"name": "Retail Sales", "observations": conv["RSAFS"][-365:]},
                  "UMCSENT": {"name": "Consumer Sentiment", "observations": conv["UMCSENT"][-365:]}
              }

          except Exception as e:
              import datetime as dt
              today = dt.date.today()
              w = [(today - dt.timedelta(days=7*i)) for i in range(7)][::-1]
              series = {
                  "T10Y3M": {"name":"10Y–3M Spread","observations":[{"date":str(d),"value":str(v)} for d,v in zip(w,[0.3,0.28,0.25,0.23,0.21,0.20,0.18])]},
                  "CREDIT": {"name":"Credit Spread","observations":[{"date":str(d),"value":str(v)} for d,v in zip(w,[0.65,0.67,0.69,0.71,0.73,0.74,0.75])]},
                  "UN_SLOPE6": {"name":"Unemp Δ6m","observations":[{"date":str(d),"value":str(v)} for d,v in zip(w,[0.1,0.12,0.14,0.15,0.17,0.18,0.2])]},
                  "DD_12M": {"name":"Drawdown","observations":[{"date":str(d),"value":str(v)} for d,v in zip(w,[-0.02,-0.03,-0.04,-0.05,-0.06,-0.07,-0.08])]},
                  "NFCI": {"name":"NFCI","observations":[{"date":str(d),"value":str(v)} for d,v in zip(w,[-0.50,-0.49,-0.47,-0.46,-0.45,-0.44,-0.43])]},
                  "VIX_PROXY": {"name":"VIX","observations":[{"date":str(d),"value":str(v)} for d,v in zip(w,[14,15,16,18,19,20,22])]},
                  "RSAFS": {"name":"Retail Sales","observations":[{"date":str(d),"value":str(v)} for d,v in zip(w,[0.3,0.31,0.32,0.33,0.34,0.35,0.36])]},
                  "UMCSENT": {"name":"Sentiment","observations":[{"date":str(d),"value":str(v)} for d,v in zip(w,[85,84,83,82,81,80,79])]}
              }

          payload = {
              "fetched_at_utc": dt.datetime.utcnow().replace(microsecond=0).isoformat()+"Z",
              "series": series
          }

          with open(OUT_FILE, "w", encoding="utf-8") as f:
              json.dump(payload, f, indent=2)

          print("✅ Wrote", OUT_FILE)
          PY

      - name: Commit cached data
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(data): update fred_cache.json"
          file_pattern: "data/fred_cache.json"
          commit_user_name: github-actions[bot]
          commit_user_email: 41898282+github-actions[bot]@users.noreply.github.com
