name: Build FRED cache (always produce valid JSON)

on:
  workflow_dispatch: {}
  schedule:
    - cron: "7 6 * * 1-5"     # 06:07 UTC on weekdays
  push:
    branches: [ main ]
    paths:
      - ".github/workflows/fred-cache.yml"
      - "index.html"
      - "data/**"

permissions:
  contents: write

jobs:
  update-cache:
    runs-on: ubuntu-latest

    steps:
      - name: Check out repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Build data/fred_cache.json
        env:
          FRED_API_KEY: ${{ secrets.FRED_API_KEY }}
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, json, datetime as dt
          from urllib.request import urlopen
          from urllib.parse import urlencode
          from collections import deque

          OUT_DIR = "data"
          OUT_FILE = os.path.join(OUT_DIR, "fred_cache.json")
          os.makedirs(OUT_DIR, exist_ok=True)

          # -------- Required series (FRED id) --------
          REQUIRED = {
              "T10Y3M":  "T10Y3M",    # Yield curve (10y - 3m)
              "CREDIT":  "BAA10YM",   # Credit spread proxy (BAA - 10y)
              "UNRATE":  "UNRATE",    # Unemployment rate (for slope derivation)
              "SP500":   "SP500",     # S&P 500 index (for drawdown derivation)
              "NFCI":    "NFCI",      # Chicago Fed NFCI
              "VIXCLS":  "VIXCLS",    # CBOE VIX (base; we'll also store VIX_PROXY)
              "RSAFS":   "RSAFS",     # Retail & Food Services Sales (for MoM in UI)
              "UMCSENT": "UMCSENT",   # Consumer Sentiment
          }

          # -------- Helpers --------
          def fred_json(series_id, api_key):
              """Fetch observations from FRED; raises on HTTP error."""
              params = dict(
                  series_id=series_id,
                  api_key=api_key,
                  file_type="json",
                  observation_start="2010-01-01"
              )
              url = "https://api.stlouisfed.org/fred/series/observations?" + urlencode(params)
              with urlopen(url, timeout=30) as r:
                  return json.load(r)

          def to_obs_list(raw):
              """Normalize FRED JSON to [{date:'YYYY-MM-DD', value:'<float as string>'}, ...]."""
              out = []
              for o in raw.get("observations", []):
                  v = o.get("value")
                  if not v or v in (".", "NaN"):
                      continue
                  try:
                      fv = float(v)
                  except Exception:
                      continue
                  out.append({"date": o["date"][:10], "value": f"{fv}"})
              return out

          def derive_un_slope6(un_obs):
              """6-month change of UNRATE using month buckets."""
              by_month = {}
              for o in un_obs:
                  by_month[o["date"][:7]] = float(o["value"])
              months = sorted(by_month)
              out = []
              for i in range(6, len(months)):
                  m = months[i]; prev = months[i-6]
                  delta = by_month[m] - by_month[prev]
                  y, mo = map(int, m.split("-"))
                  # month-end date
                  if mo == 12:
                      d = dt.date(y, 12, 31)
                  else:
                      d = dt.date(y, mo+1, 1) - dt.timedelta(days=1)
                  out.append({"date": str(d), "value": f"{delta}"})
              return out

          def derive_drawdown_12m(sp_obs):
              """Rolling 12m drawdown using a 252-trading-day rolling max (O(n) deque)."""
              if not sp_obs:
                  return []
              vals = [float(o["value"]) for o in sp_obs]
              dates = [o["date"] for o in sp_obs]
              win = 252
              dq = deque()          # indices, maintaining decreasing values
              peak = [None]*len(vals)

              for i, v in enumerate(vals):
                  while dq and dq[0] <= i - win:
                      dq.popleft()
                  while dq and vals[dq[-1]] <= v:
                      dq.pop()
                  dq.append(i)
                  peak[i] = vals[dq[0]]

              out = []
              for d, v, p in zip(dates, vals, peak):
                  if p and p != 0:
                      out.append({"date": d, "value": f"{(v/p) - 1.0}"})
              return out

          # -------- Fetch from FRED (or raise if API missing) --------
          api_key = os.environ.get("FRED_API_KEY", "").strip()
          if not api_key:
              raise SystemExit("FRED_API_KEY secret is not set.")

          raw = {}
          for key, fred_id in REQUIRED.items():
              raw[key] = fred_json(fred_id, api_key)

          # Normalize observations
          obs = {k: to_obs_list(raw[k]) for k in raw}

          # Derivations needed by the UI
          un_slope6 = derive_un_slope6(obs["UNRATE"])
          dd_12m    = derive_drawdown_12m(obs["SP500"])

          # Build the cache object
          series = {
              "T10Y3M":   {"name": "10Y minus 3M Treasury Spread", "observations": obs["T10Y3M"]},
              "CREDIT":   {"name": "Credit Spread (BAA - 10Y)",    "observations": obs["CREDIT"]},
              "UNRATE":   {"name": "Unemployment Rate",            "observations": obs["UNRATE"]},
              "UN_SLOPE6":{"name": "Unemployment 6m Slope",        "observations": un_slope6},
              "SP500":    {"name": "S&P 500",                      "observations": obs["SP500"]},
              "DD_12M":   {"name": "S&P 500 12m Drawdown",         "observations": dd_12m},
              "NFCI":     {"name": "Chicago Fed NFCI",             "observations": obs["NFCI"]},
              "VIXCLS":   {"name": "CBOE Volatility Index",        "observations": obs["VIXCLS"]},
              "VIX_PROXY":{"name": "Volatility Index (proxy)",     "observations": obs["VIXCLS"]},  # duplicate for convenience
              "RSAFS":    {"name": "Retail & Food Services Sales", "observations": obs["RSAFS"]},
              "UMCSENT":  {"name": "Consumer Sentiment",           "observations": obs["UMCSENT"]},
          }

          payload = {
              "fetched_at_utc": dt.datetime.utcnow().replace(microsecond=0).isoformat() + "Z",
              "series": series
          }

          with open(OUT_FILE, "w", encoding="utf-8") as f:
              json.dump(payload, f, indent=2)

          print(f"âœ… Wrote {OUT_FILE} with keys:", ", ".join(sorted(series.keys())))
          PY

      - name: Commit cached data
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "chore(data): update fred_cache.json"
          file_pattern: "data/fred_cache.json"
          commit_user_name: github-actions[bot]
          commit_user_email: 41898282+github-actions[bot]@users.noreply.github.com
